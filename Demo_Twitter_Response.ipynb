{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "### Methodology:\n",
    "I first applied LSTM and BiLSTM but they both have serious overfitting problem. After some parameter tunning and no significant improvement on the results, I decided that it is critical to perform proper data preprocessing.\n",
    "\n",
    "I have implemented three methods to preprocess the data:\n",
    "* Word2vec\n",
    "* Embedding from GloVe\n",
    "* Embedding on the given dataset\n",
    "\n",
    "The last embedding method works the best in improving the accuracy.\n",
    "\n",
    "Then I implemented LSTM and BiLSTM to perdict sentiment based on the proprocessed tweets.\n",
    "\n",
    "### LSTM vs BiLSTM Result\n",
    "I used accuracy as the metric since the number of positive and negative tweets are roughly equal. Without further instruction, I assume we want the general classification accuracy but not precision and recall. \n",
    "After proper data preprocessing and embedding, the accuracy of bi-directional LSTM and vanilla LSTM are quite close. \n",
    "* With word2vec the accuracy for LSTM and BiLSTM are 0.6562 and 0.6666. \n",
    "* With GloVe embedding the accuracy for LSTM and BiLSTM are 0.5614 and 0.6024.\n",
    "* With embedding trained on this given dataset the accuracy for LSTM and BiLSTM are 0.7671 and 0.7669.\n",
    "\n",
    "Little difference in the performance is because that we are predicting sentiment on the whole tweets text.\n",
    "\n",
    "### LSTM vs RNN\n",
    "The advantage of using an LSTM over a vanilla RNN is that LSTM can maintain long-term information by training the memory cell. While both methods train on a sequential dataset, a vanilla RNN easily losses long-term information due to vanishing gradients. The sigmoid function in forget and input/output gates ensures that the small gradients in between two highly correlated words would not corrupt the relationship. With the gates, LSTM achieves higher prediction accuracy than a vanilla RNN in text analysis.\n",
    "\n",
    "### BiLSTM vs LSTM \n",
    "The advantage of using a bi-directional LSTM is that it learns additional future information with the whole context. BiLSTM combines the parameters trained from a positive and a reversed sequential direction to make predictions. Therefore, it preserves information from both the past and the future while a vanilla LSTM only predicts based on the past. \n",
    "BiLSTM is applicable on this dataset because we are not performing online training on the tweets dataset and have the whole tweet content available before training.\n",
    "\n",
    "### Future Work\n",
    "Futher work to improve the accuracy includes increasing training epochs, fine-tunning and further examination of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 11:50:17,044 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from preprocess_data import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load And Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 500000/500000 [00:00<00:00, 610496.70it/s]\n",
      "2018-11-12 11:50:20,348 : INFO : Encoded Labels\n",
      "progress-bar: 100%|██████████| 500000/500000 [02:42<00:00, 3067.75it/s]\n",
      "2018-11-12 11:53:03,363 : INFO : Cleaned Tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@mileycyrus http://twitpic.com/4fzo7 - noooo d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mileycyrus, 4fzo7, noooo, dont, dye, pretti, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@accentuations ty m'dear!</td>\n",
       "      <td>1</td>\n",
       "      <td>[accentu, mdear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Shauna_nkotb_ca I feel so bad for the Oz girl...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shaunankotbca, feel, bad, girlsthat, realli, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm going to run away from home... I'm planing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[go, run, away, home, plane, escap, cant, wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@seansparks hah that's awesome  I had one of t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[seanspark, hah, that, awesom, one, son, hehe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text  sentiment  \\\n",
       "0       0  @mileycyrus http://twitpic.com/4fzo7 - noooo d...          0   \n",
       "1       4                         @accentuations ty m'dear!           1   \n",
       "2       0  @Shauna_nkotb_ca I feel so bad for the Oz girl...          0   \n",
       "3       0  I'm going to run away from home... I'm planing...          0   \n",
       "4       4  @seansparks hah that's awesome  I had one of t...          1   \n",
       "\n",
       "                                              tokens  \n",
       "0  [mileycyrus, 4fzo7, noooo, dont, dye, pretti, ...  \n",
       "1                                   [accentu, mdear]  \n",
       "2  [shaunankotbca, feel, bad, girlsthat, realli, ...  \n",
       "3  [go, run, away, home, plane, escap, cant, wait...  \n",
       "4     [seanspark, hah, that, awesom, one, son, hehe]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/sentiment_data.csv', encoding='latin1')\n",
    "processed_data = preprocess_data(df)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contruct Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(processed_data['tokens']),\n",
    "                                                    np.array(processed_data['sentiment']), test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We will use 3 encoding method:1. self_trained word2vec 2. glove embedding 3. embedding layer. For each method, we will also train single layer lstm and bidirectional lstm seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350000/350000 [00:00<00:00, 2300434.86it/s]\n",
      "2018-11-12 11:53:03,657 : INFO : collecting all words and their counts\n",
      "2018-11-12 11:53:03,657 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-12 11:53:03,690 : INFO : PROGRESS: at sentence #10000, processed 74016 words, keeping 17205 word types\n",
      "2018-11-12 11:53:03,720 : INFO : PROGRESS: at sentence #20000, processed 147965 words, keeping 28629 word types\n",
      "2018-11-12 11:53:03,752 : INFO : PROGRESS: at sentence #30000, processed 220670 words, keeping 38522 word types\n",
      "2018-11-12 11:53:03,783 : INFO : PROGRESS: at sentence #40000, processed 294211 words, keeping 47802 word types\n",
      "2018-11-12 11:53:03,815 : INFO : PROGRESS: at sentence #50000, processed 367756 words, keeping 56723 word types\n",
      "2018-11-12 11:53:03,851 : INFO : PROGRESS: at sentence #60000, processed 441403 words, keeping 65215 word types\n",
      "2018-11-12 11:53:03,879 : INFO : PROGRESS: at sentence #70000, processed 514603 words, keeping 73508 word types\n",
      "2018-11-12 11:53:03,913 : INFO : PROGRESS: at sentence #80000, processed 588180 words, keeping 81272 word types\n",
      "2018-11-12 11:53:03,946 : INFO : PROGRESS: at sentence #90000, processed 661728 words, keeping 89040 word types\n",
      "2018-11-12 11:53:03,981 : INFO : PROGRESS: at sentence #100000, processed 734398 words, keeping 96350 word types\n",
      "2018-11-12 11:53:04,016 : INFO : PROGRESS: at sentence #110000, processed 807473 words, keeping 103664 word types\n",
      "2018-11-12 11:53:04,048 : INFO : PROGRESS: at sentence #120000, processed 880692 words, keeping 110901 word types\n",
      "2018-11-12 11:53:04,078 : INFO : PROGRESS: at sentence #130000, processed 954597 words, keeping 117853 word types\n",
      "2018-11-12 11:53:04,110 : INFO : PROGRESS: at sentence #140000, processed 1028247 words, keeping 124734 word types\n",
      "2018-11-12 11:53:04,147 : INFO : PROGRESS: at sentence #150000, processed 1101826 words, keeping 131497 word types\n",
      "2018-11-12 11:53:04,181 : INFO : PROGRESS: at sentence #160000, processed 1174796 words, keeping 137995 word types\n",
      "2018-11-12 11:53:04,210 : INFO : PROGRESS: at sentence #170000, processed 1247812 words, keeping 144435 word types\n",
      "2018-11-12 11:53:04,246 : INFO : PROGRESS: at sentence #180000, processed 1321537 words, keeping 150914 word types\n",
      "2018-11-12 11:53:04,280 : INFO : PROGRESS: at sentence #190000, processed 1395365 words, keeping 157243 word types\n",
      "2018-11-12 11:53:04,311 : INFO : PROGRESS: at sentence #200000, processed 1468901 words, keeping 163396 word types\n",
      "2018-11-12 11:53:04,340 : INFO : PROGRESS: at sentence #210000, processed 1542858 words, keeping 169623 word types\n",
      "2018-11-12 11:53:04,376 : INFO : PROGRESS: at sentence #220000, processed 1616425 words, keeping 175645 word types\n",
      "2018-11-12 11:53:04,406 : INFO : PROGRESS: at sentence #230000, processed 1689103 words, keeping 181721 word types\n",
      "2018-11-12 11:53:04,438 : INFO : PROGRESS: at sentence #240000, processed 1762892 words, keeping 187614 word types\n",
      "2018-11-12 11:53:04,469 : INFO : PROGRESS: at sentence #250000, processed 1836375 words, keeping 193461 word types\n",
      "2018-11-12 11:53:04,499 : INFO : PROGRESS: at sentence #260000, processed 1909532 words, keeping 199342 word types\n",
      "2018-11-12 11:53:04,535 : INFO : PROGRESS: at sentence #270000, processed 1982509 words, keeping 204946 word types\n",
      "2018-11-12 11:53:04,566 : INFO : PROGRESS: at sentence #280000, processed 2056556 words, keeping 210721 word types\n",
      "2018-11-12 11:53:04,597 : INFO : PROGRESS: at sentence #290000, processed 2130139 words, keeping 216293 word types\n",
      "2018-11-12 11:53:04,628 : INFO : PROGRESS: at sentence #300000, processed 2202822 words, keeping 221823 word types\n",
      "2018-11-12 11:53:04,660 : INFO : PROGRESS: at sentence #310000, processed 2276236 words, keeping 227273 word types\n",
      "2018-11-12 11:53:04,690 : INFO : PROGRESS: at sentence #320000, processed 2349703 words, keeping 232718 word types\n",
      "2018-11-12 11:53:04,721 : INFO : PROGRESS: at sentence #330000, processed 2423007 words, keeping 238198 word types\n",
      "2018-11-12 11:53:04,754 : INFO : PROGRESS: at sentence #340000, processed 2496786 words, keeping 243569 word types\n",
      "2018-11-12 11:53:04,785 : INFO : collected 248848 word types from a corpus of 2569896 raw words and 350000 sentences\n",
      "2018-11-12 11:53:04,786 : INFO : Loading a fresh vocabulary\n",
      "2018-11-12 11:53:04,932 : INFO : effective_min_count=10 retains 11908 unique words (4% of original 248848, drops 236940)\n",
      "2018-11-12 11:53:04,933 : INFO : effective_min_count=10 leaves 2217643 word corpus (86% of original 2569896, drops 352253)\n",
      "2018-11-12 11:53:04,986 : INFO : deleting the raw counts dictionary of 248848 items\n",
      "2018-11-12 11:53:04,994 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2018-11-12 11:53:04,994 : INFO : downsampling leaves estimated 2018179 word corpus (91.0% of prior 2217643)\n",
      "2018-11-12 11:53:05,034 : INFO : estimated required memory for 11908 words and 200 dimensions: 25006800 bytes\n",
      "2018-11-12 11:53:05,035 : INFO : resetting layer weights\n",
      "100%|██████████| 350000/350000 [00:00<00:00, 2673302.06it/s]\n",
      "2018-11-12 11:53:05,385 : INFO : training model with 3 workers on 11908 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-12 11:53:06,394 : INFO : EPOCH 1 - PROGRESS: at 55.21% examples, 1114158 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-12 11:53:07,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-12 11:53:07,191 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-12 11:53:07,196 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-12 11:53:07,197 : INFO : EPOCH - 1 : training on 2569896 raw words (2017846 effective words) took 1.8s, 1119283 effective words/s\n",
      "2018-11-12 11:53:08,217 : INFO : EPOCH 2 - PROGRESS: at 52.51% examples, 1055273 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-12 11:53:09,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-12 11:53:09,101 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-12 11:53:09,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-12 11:53:09,106 : INFO : EPOCH - 2 : training on 2569896 raw words (2018247 effective words) took 1.9s, 1065990 effective words/s\n",
      "2018-11-12 11:53:10,119 : INFO : EPOCH 3 - PROGRESS: at 53.67% examples, 1077331 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-12 11:53:10,955 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-12 11:53:10,961 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-12 11:53:10,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-12 11:53:10,966 : INFO : EPOCH - 3 : training on 2569896 raw words (2018986 effective words) took 1.9s, 1089830 effective words/s\n",
      "2018-11-12 11:53:11,983 : INFO : EPOCH 4 - PROGRESS: at 54.06% examples, 1090598 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-12 11:53:12,803 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-12 11:53:12,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-12 11:53:12,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-12 11:53:12,813 : INFO : EPOCH - 4 : training on 2569896 raw words (2018021 effective words) took 1.8s, 1102076 effective words/s\n",
      "2018-11-12 11:53:13,838 : INFO : EPOCH 5 - PROGRESS: at 55.21% examples, 1104215 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-12 11:53:14,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-12 11:53:14,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-12 11:53:14,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-12 11:53:14,671 : INFO : EPOCH - 5 : training on 2569896 raw words (2017962 effective words) took 1.8s, 1095303 effective words/s\n",
      "2018-11-12 11:53:14,672 : INFO : training on a 12849480 raw words (10091062 effective words) took 9.3s, 1086733 effective words/s\n",
      "100%|██████████| 350000/350000 [00:35<00:00, 9724.30it/s] \n",
      "2018-11-12 11:53:54,086 : INFO : word2vec generated\n",
      "100%|██████████| 150000/150000 [00:15<00:00, 9582.45it/s]\n",
      "2018-11-12 11:54:10,921 : INFO : word2vec generated\n"
     ]
    }
   ],
   "source": [
    "# train word2vec\n",
    "tweet_w2v = word2vec_build(x_train, dim = 200)\n",
    "# get word2vec representation for x_train and y_train\n",
    "train_vecs = word2vec_generate(x_train, tweet_w2v, dim = 200)\n",
    "test_vecs = word2vec_generate(x_test, tweet_w2v, dim = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "350000/350000 [==============================] - 1102s 3ms/step - loss: 0.6174 - acc: 0.6562\n",
      "150000/150000 [==============================] - 80s 533us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 12:13:54,739 : INFO : Test Accuracy 68.11800000063577%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc8ba97d0b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_layer_lstm_w2v(train_vecs, y_train, test_vecs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "350000/350000 [==============================] - 1498s 4ms/step - loss: 0.6054 - acc: 0.6666\n",
      "150000/150000 [==============================] - 81s 543us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 12:40:16,428 : INFO : Test Accuracy 69.5533333346049%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc82478d7b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidirectional_lstm_w2v(train_vecs, y_train, test_vecs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 12:52:50,012 : INFO : converting 400000 vectors from data/glove.6B.100d.txt to data/gensim_glove_vectors.txt\n",
      "2018-11-12 12:52:51,556 : INFO : loading projection weights from data/gensim_glove_vectors.txt\n",
      "2018-11-12 12:53:32,059 : INFO : loaded (400000, 100) matrix from data/gensim_glove_vectors.txt\n",
      "100%|██████████| 350000/350000 [00:18<00:00, 18951.02it/s]\n",
      "2018-11-12 12:53:52,282 : INFO : word2vec generated\n",
      "100%|██████████| 150000/150000 [00:07<00:00, 19266.74it/s]\n",
      "2018-11-12 12:54:00,705 : INFO : word2vec generated\n"
     ]
    }
   ],
   "source": [
    "glove2word2vec('data/glove.6B.100d.txt', 'data/gensim_glove_vectors.txt')\n",
    "glove_embedding = KeyedVectors.load_word2vec_format('data/gensim_glove_vectors.txt')\n",
    "train_vecs = word2vec_generate(x_train, glove_embedding, dim = 100)\n",
    "test_vecs = word2vec_generate(x_test, glove_embedding, dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "350000/350000 [==============================] - 597s 2ms/step - loss: 0.6783 - acc: 0.5614\n",
      "150000/150000 [==============================] - 40s 267us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 13:04:38,774 : INFO : Test Accuracy 57.8873333346049%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc7cfb8bda0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_layer_lstm_w2v(train_vecs, y_train, test_vecs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "350000/350000 [==============================] - 781s 2ms/step - loss: 0.6591 - acc: 0.6024\n",
      "150000/150000 [==============================] - 44s 292us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 13:18:25,866 : INFO : Test Accuracy 62.37066666793824%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc7b5ed7eb8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidirectional_lstm_w2v(train_vecs, y_train, test_vecs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_pad = create_sequence(x_train)\n",
    "x_test_pad = create_sequence(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "350000/350000 [==============================] - 625s 2ms/step - loss: 0.4818 - acc: 0.7671\n",
      "150000/150000 [==============================] - 28s 185us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 13:29:30,527 : INFO : Test Accuracy 54.316000000635775%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc83a19e780>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_layer_lstm_embedding(x_train_pad, y_train, x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "350000/350000 [==============================] - 813s 2ms/step - loss: 0.4821 - acc: 0.7669\n",
      "150000/150000 [==============================] - 41s 273us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 13:43:46,946 : INFO : Test Accuracy 54.413999999364215%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fc836eda2b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidirectional_lstm_embedding(x_train_pad, y_train, x_test_pad, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
